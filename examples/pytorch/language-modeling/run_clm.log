$ python run_clm.py*
2024-08-19 09:07:41.481713: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-19 09:07:41.481772: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-19 09:07:41.517038: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-19 09:07:42.781037: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)

MODEL_CONFIG_CLASSES: [
    <class 'transformers.models.bart.configuration_bart.BartConfig'>,
    <class 'transformers.models.bert.configuration_bert.BertConfig'>,
    <class 'transformers.models.bert_generation.configuration_bert_generation.BertGenerationConfig'>,
    <class 'transformers.models.big_bird.configuration_big_bird.BigBirdConfig'>,
    <class 'transformers.models.bigbird_pegasus.configuration_bigbird_pegasus.BigBirdPegasusConfig'>,
    <class 'transformers.models.biogpt.configuration_biogpt.BioGptConfig'>,
    <class 'transformers.models.blenderbot.configuration_blenderbot.BlenderbotConfig'>,
    <class 'transformers.models.blenderbot_small.configuration_blenderbot_small.BlenderbotSmallConfig'>,
    <class 'transformers.models.bloom.configuration_bloom.BloomConfig'>,
    <class 'transformers.models.camembert.configuration_camembert.CamembertConfig'>,
    <class 'transformers.models.llama.configuration_llama.LlamaConfig'>,
    <class 'transformers.models.codegen.configuration_codegen.CodeGenConfig'>,
    <class 'transformers.models.cpmant.configuration_cpmant.CpmAntConfig'>,
    <class 'transformers.models.ctrl.configuration_ctrl.CTRLConfig'>,
    <class 'transformers.models.data2vec.configuration_data2vec_text.Data2VecTextConfig'>,
    <class 'transformers.models.electra.configuration_electra.ElectraConfig'>,
    <class 'transformers.models.ernie.configuration_ernie.ErnieConfig'>,
    <class 'transformers.models.falcon.configuration_falcon.FalconConfig'>,
    <class 'transformers.models.fuyu.configuration_fuyu.FuyuConfig'>,
    <class 'transformers.models.gemma.configuration_gemma.GemmaConfig'>,
    <class 'transformers.models.git.configuration_git.GitConfig'>,
    <class 'transformers.models.gpt2.configuration_gpt2.GPT2Config'>,
    <class 'transformers.models.gpt2.configuration_gpt2.GPT2Config'>,
    <class 'transformers.models.gpt_bigcode.configuration_gpt_bigcode.GPTBigCodeConfig'>,
    <class 'transformers.models.gpt_neo.configuration_gpt_neo.GPTNeoConfig'>,
    <class 'transformers.models.gpt_neox.configuration_gpt_neox.GPTNeoXConfig'>,
    <class 'transformers.models.gpt_neox_japanese.configuration_gpt_neox_japanese.GPTNeoXJapaneseConfig'>,
    <class 'transformers.models.gptj.configuration_gptj.GPTJConfig'>,
    <class 'transformers.models.llama.configuration_llama.LlamaConfig'>,
    <class 'transformers.models.marian.configuration_marian.MarianConfig'>,
    <class 'transformers.models.mbart.configuration_mbart.MBartConfig'>,
    <class 'transformers.models.mega.configuration_mega.MegaConfig'>,
    <class 'transformers.models.megatron_bert.configuration_megatron_bert.MegatronBertConfig'>,
    <class 'transformers.models.mistral.configuration_mistral.MistralConfig'>,
    <class 'transformers.models.mixtral.configuration_mixtral.MixtralConfig'>,
    <class 'transformers.models.mpt.configuration_mpt.MptConfig'>,
    <class 'transformers.models.musicgen.configuration_musicgen.MusicgenConfig'>,
    <class 'transformers.models.mvp.configuration_mvp.MvpConfig'>,
    <class 'transformers.models.deprecated.open_llama.configuration_open_llama.OpenLlamaConfig'>,
    <class 'transformers.models.openai.configuration_openai.OpenAIGPTConfig'>,
    <class 'transformers.models.opt.configuration_opt.OPTConfig'>,
    <class 'transformers.models.pegasus.configuration_pegasus.PegasusConfig'>,
    <class 'transformers.models.persimmon.configuration_persimmon.PersimmonConfig'>,
    <class 'transformers.models.phi.configuration_phi.PhiConfig'>,
    <class 'transformers.models.plbart.configuration_plbart.PLBartConfig'>,
    <class 'transformers.models.prophetnet.configuration_prophetnet.ProphetNetConfig'>,
    <class 'transformers.models.qdqbert.configuration_qdqbert.QDQBertConfig'>,
    <class 'transformers.models.qwen2.configuration_qwen2.Qwen2Config'>,
    <class 'transformers.models.reformer.configuration_reformer.ReformerConfig'>,
    <class 'transformers.models.rembert.configuration_rembert.RemBertConfig'>,
    <class 'transformers.models.roberta.configuration_roberta.RobertaConfig'>,
    <class 'transformers.models.roberta_prelayernorm.configuration_roberta_prelayernorm.RobertaPreLayerNormConfig'>,
    <class 'transformers.models.roc_bert.configuration_roc_bert.RoCBertConfig'>,
    <class 'transformers.models.roformer.configuration_roformer.RoFormerConfig'>,
    <class 'transformers.models.rwkv.configuration_rwkv.RwkvConfig'>,
    <class 'transformers.models.speech_to_text_2.configuration_speech_to_text_2.Speech2Text2Config'>,
    <class 'transformers.models.stablelm.configuration_stablelm.StableLmConfig'>,
    <class 'transformers.models.deprecated.transfo_xl.configuration_transfo_xl.TransfoXLConfig'>,
    <class 'transformers.models.trocr.configuration_trocr.TrOCRConfig'>,
    <class 'transformers.models.whisper.configuration_whisper.WhisperConfig'>,
    <class 'transformers.models.xglm.configuration_xglm.XGLMConfig'>,
    <class 'transformers.models.xlm.configuration_xlm.XLMConfig'>,
    <class 'transformers.models.xlm_prophetnet.configuration_xlm_prophetnet.XLMProphetNetConfig'>,
    <class 'transformers.models.xlm_roberta.configuration_xlm_roberta.XLMRobertaConfig'>,
    <class 'transformers.models.xlm_roberta_xl.configuration_xlm_roberta_xl.XLMRobertaXLConfig'>,
    <class 'transformers.models.xlnet.configuration_xlnet.XLNetConfig'>,
    <class 'transformers.models.xmod.configuration_xmod.XmodConfig'>
]

MODEL_TYPES: (
    'bart',
    'bert',
    'bert-generation',
    'big_bird',
    'bigbird_pegasus',
    'biogpt',
    'blenderbot',
    'blenderbot-small',
    'bloom',
    'camembert',
    'llama',
    'codegen',
    'cpmant',
    'ctrl',
    'data2vec-text',
    'electra',
    'ernie',
    'falcon',
    'fuyu',
    'gemma',
    'git',
    'gpt2',
    'gpt2',
    'gpt_bigcode',
    'gpt_neo',
    'gpt_neox',
    'gpt_neox_japanese',
    'gptj',
    'llama',
    'marian',
    'mbart',
    'mega',
    'megatron-bert',
    'mistral',
    'mixtral',
    'mpt',
    'musicgen',
    'mvp',
    'open-llama',
    'openai-gpt',
    'opt',
    'pegasus',
    'persimmon',
    'phi',
    'plbart',
    'prophetnet',
    'qdqbert',
    'qwen2',
    'reformer',
    'rembert',
    'roberta',
    'roberta-prelayernorm',
    'roc_bert',
    'roformer',
    'rwkv',
    'speech_to_text_2',
    'stablelm',
    'transfo-xl',
    'trocr',
    'whisper',
    'xglm',
    'xlm',
    'xlm-prophetnet',
    'xlm-roberta',
    'xlm-roberta-xl',
    'xlnet',
    'xmod'
)

usage: run_clm.py*
[-h]
[--model_name_or_path MODEL_NAME_OR_PATH]
[--model_type MODEL_TYPE]
[--config_overrides CONFIG_OVERRIDES]
[--config_name CONFIG_NAME]
[--tokenizer_name TOKENIZER_NAME]
[--cache_dir CACHE_DIR]
[--use_fast_tokenizer [USE_FAST_TOKENIZER]] [--no_use_fast_tokenizer]
[--model_revision MODEL_REVISION]
[--token TOKEN]
[--use_auth_token [USE_AUTH_TOKEN]]
[--trust_remote_code [TRUST_REMOTE_CODE]]
[--torch_dtype {auto,bfloat16,float16,float32}]
[--low_cpu_mem_usage [LOW_CPU_MEM_USAGE]]

[--dataset_name DATASET_NAME]
[--dataset_config_name DATASET_CONFIG_NAME]
[--train_file TRAIN_FILE]
[--validation_file VALIDATION_FILE]
[--max_train_samples MAX_TRAIN_SAMPLES]
[--max_eval_samples MAX_EVAL_SAMPLES]
[--streaming [STREAMING]]
[--block_size BLOCK_SIZE]
[--overwrite_cache [OVERWRITE_CACHE]]
[--validation_split_percentage VALIDATION_SPLIT_PERCENTAGE]
[--preprocessing_num_workers PREPROCESSING_NUM_WORKERS]
[--keep_linebreaks [KEEP_LINEBREAKS]] [--no_keep_linebreaks]

--output_dir OUTPUT_DIR
[--overwrite_output_dir [OVERWRITE_OUTPUT_DIR]]
[--do_train [DO_TRAIN]]
[--do_eval [DO_EVAL]]
[--do_predict [DO_PREDICT]]
[--evaluation_strategy {no,steps,epoch}]
[--prediction_loss_only [PREDICTION_LOSS_ONLY]]
[--per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE]
[--per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE]
[--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]
[--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]
[--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]
[--eval_accumulation_steps EVAL_ACCUMULATION_STEPS]
[--eval_delay EVAL_DELAY]
[--learning_rate LEARNING_RATE]
[--weight_decay WEIGHT_DECAY]
[--adam_beta1 ADAM_BETA1]
[--adam_beta2 ADAM_BETA2]
[--adam_epsilon ADAM_EPSILON]
[--max_grad_norm MAX_GRAD_NORM]
[--num_train_epochs NUM_TRAIN_EPOCHS]
[--max_steps MAX_STEPS]
[--lr_scheduler_type {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup,inverse_sqrt,reduce_lr_on_plateau}]
[--lr_scheduler_kwargs LR_SCHEDULER_KWARGS]
[--warmup_ratio WARMUP_RATIO]
[--warmup_steps WARMUP_STEPS]
[--log_level {detail,debug,info,warning,error,critical,passive}]
[--log_level_replica {detail,debug,info,warning,error,critical,passive}]
[--log_on_each_node [LOG_ON_EACH_NODE]] [--no_log_on_each_node]
[--logging_dir LOGGING_DIR]
[--logging_strategy {no,steps,epoch}]
[--logging_first_step [LOGGING_FIRST_STEP]]
[--logging_steps LOGGING_STEPS]
[--logging_nan_inf_filter [LOGGING_NAN_INF_FILTER]] [--no_logging_nan_inf_filter]
[--save_strategy {no,steps,epoch}]
[--save_steps SAVE_STEPS]
[--save_total_limit SAVE_TOTAL_LIMIT]
[--save_safetensors [SAVE_SAFETENSORS]] [--no_save_safetensors]
[--save_on_each_node [SAVE_ON_EACH_NODE]]
[--save_only_model [SAVE_ONLY_MODEL]]
[--no_cuda [NO_CUDA]]
[--use_cpu [USE_CPU]]
[--use_mps_device [USE_MPS_DEVICE]]
[--seed SEED]
[--data_seed DATA_SEED]
[--jit_mode_eval [JIT_MODE_EVAL]]
[--use_ipex [USE_IPEX]]
[--bf16 [BF16]]
[--fp16 [FP16]]
[--fp16_opt_level FP16_OPT_LEVEL]
[--half_precision_backend {auto,apex,cpu_amp}]
[--bf16_full_eval [BF16_FULL_EVAL]]
[--fp16_full_eval [FP16_FULL_EVAL]]
[--tf32 TF32]
[--local_rank LOCAL_RANK]
[--ddp_backend {nccl,gloo,mpi,ccl,hccl}]
[--tpu_num_cores TPU_NUM_CORES]
[--tpu_metrics_debug [TPU_METRICS_DEBUG]]
[--debug DEBUG [DEBUG ...]]
[--dataloader_drop_last [DATALOADER_DROP_LAST]]
[--eval_steps EVAL_STEPS]
[--dataloader_num_workers DATALOADER_NUM_WORKERS]
[--dataloader_prefetch_factor DATALOADER_PREFETCH_FACTOR]
[--past_index PAST_INDEX]
[--run_name RUN_NAME]
[--disable_tqdm DISABLE_TQDM]
[--remove_unused_columns [REMOVE_UNUSED_COLUMNS]] [--no_remove_unused_columns]
[--label_names LABEL_NAMES [LABEL_NAMES ...]]
[--load_best_model_at_end [LOAD_BEST_MODEL_AT_END]]
[--metric_for_best_model METRIC_FOR_BEST_MODEL]
[--greater_is_better GREATER_IS_BETTER]
[--ignore_data_skip [IGNORE_DATA_SKIP]]
[--fsdp FSDP]
[--fsdp_min_num_params FSDP_MIN_NUM_PARAMS]
[--fsdp_config FSDP_CONFIG]
[--fsdp_transformer_layer_cls_to_wrap FSDP_TRANSFORMER_LAYER_CLS_TO_WRAP]
[--accelerator_config ACCELERATOR_CONFIG]
[--deepspeed DEEPSPEED]
[--label_smoothing_factor LABEL_SMOOTHING_FACTOR]
[--optim {adamw_hf,adamw_torch,adamw_torch_fused,adamw_torch_xla,adamw_torch_npu_fused,adamw_apex_fused,adafactor,adamw_anyprecision,sgd,adagrad,adamw_bnb_8bit,adamw_8bit,lion_8bit,lion_32bit,paged_adamw_32bit,paged_adamw_8bit,paged_lion_32bit,paged_lion_8bit,rmsprop,rmsprop_bnb,rmsprop_bnb_8bit,rmsprop_bnb_32bit}]
[--optim_args OPTIM_ARGS]
[--adafactor [ADAFACTOR]]
[--group_by_length [GROUP_BY_LENGTH]]
[--length_column_name LENGTH_COLUMN_NAME]
[--report_to REPORT_TO [REPORT_TO ...]]
[--ddp_find_unused_parameters DDP_FIND_UNUSED_PARAMETERS]
[--ddp_bucket_cap_mb DDP_BUCKET_CAP_MB]
[--ddp_broadcast_buffers DDP_BROADCAST_BUFFERS]
[--dataloader_pin_memory [DATALOADER_PIN_MEMORY]] [--no_dataloader_pin_memory]
[--dataloader_persistent_workers [DATALOADER_PERSISTENT_WORKERS]]
[--skip_memory_metrics [SKIP_MEMORY_METRICS]] [--no_skip_memory_metrics]
[--use_legacy_prediction_loop [USE_LEGACY_PREDICTION_LOOP]]
[--push_to_hub [PUSH_TO_HUB]]
[--resume_from_checkpoint RESUME_FROM_CHECKPOINT]
[--hub_model_id HUB_MODEL_ID]
[--hub_strategy {end,every_save,checkpoint,all_checkpoints}]
[--hub_token HUB_TOKEN]
[--hub_private_repo [HUB_PRIVATE_REPO]]
[--hub_always_push [HUB_ALWAYS_PUSH]]
[--gradient_checkpointing [GRADIENT_CHECKPOINTING]]
[--gradient_checkpointing_kwargs GRADIENT_CHECKPOINTING_KWARGS]
[--include_inputs_for_metrics [INCLUDE_INPUTS_FOR_METRICS]]
[--fp16_backend {auto,apex,cpu_amp}]
[--push_to_hub_model_id PUSH_TO_HUB_MODEL_ID]
[--push_to_hub_organization PUSH_TO_HUB_ORGANIZATION]
[--push_to_hub_token PUSH_TO_HUB_TOKEN]
[--mp_parameters MP_PARAMETERS]
[--auto_find_batch_size [AUTO_FIND_BATCH_SIZE]]
[--full_determinism [FULL_DETERMINISM]]
[--torchdynamo TORCHDYNAMO]
[--ray_scope RAY_SCOPE]
[--ddp_timeout DDP_TIMEOUT]
[--torch_compile [TORCH_COMPILE]]
[--torch_compile_backend TORCH_COMPILE_BACKEND]
[--torch_compile_mode TORCH_COMPILE_MODE]
[--dispatch_batches DISPATCH_BATCHES]
[--split_batches SPLIT_BATCHES]
[--include_tokens_per_second [INCLUDE_TOKENS_PER_SECOND]]
[--include_num_input_tokens_seen [INCLUDE_NUM_INPUT_TOKENS_SEEN]]
[--neftune_noise_alpha NEFTUNE_NOISE_ALPHA]

run_clm.py*: error: the following arguments are required: --output_dir
$


$ python run_clm.py \
--model_name_or_path openai-community/gpt2 \
--dataset_name wikitext \
--dataset_config_name wikitext-2-raw-v1 \
--per_device_train_batch_size 8 \
--per_device_eval_batch_size 8 \
--do_train \
--do_eval \
--output_dir /tmp/test-clm

2024-08-19 10:12:10.061654: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-19 10:12:10.061771: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-19 10:12:10.348188: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-19 10:12:13.957486: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)

MODEL_CONFIG_CLASSES: [<class 'transformers.models.bart.configuration_bart.BartConfig'>, <class 'transformers.models.bert.configuration_bert.BertConfig'>, <class 'transformers.models.bert_generation.configuration_bert_generation.BertGenerationConfig'>, <class 'transformers.models.big_bird.configuration_big_bird.BigBirdConfig'>, <class 'transformers.models.bigbird_pegasus.configuration_bigbird_pegasus.BigBirdPegasusConfig'>, <class 'transformers.models.biogpt.configuration_biogpt.BioGptConfig'>, <class 'transformers.models.blenderbot.configuration_blenderbot.BlenderbotConfig'>, <class 'transformers.models.blenderbot_small.configuration_blenderbot_small.BlenderbotSmallConfig'>, <class 'transformers.models.bloom.configuration_bloom.BloomConfig'>, <class 'transformers.models.camembert.configuration_camembert.CamembertConfig'>, <class 'transformers.models.llama.configuration_llama.LlamaConfig'>, <class 'transformers.models.codegen.configuration_codegen.CodeGenConfig'>, <class 'transformers.models.cpmant.configuration_cpmant.CpmAntConfig'>, <class 'transformers.models.ctrl.configuration_ctrl.CTRLConfig'>, <class 'transformers.models.data2vec.configuration_data2vec_text.Data2VecTextConfig'>, <class 'transformers.models.electra.configuration_electra.ElectraConfig'>, <class 'transformers.models.ernie.configuration_ernie.ErnieConfig'>, <class 'transformers.models.falcon.configuration_falcon.FalconConfig'>, <class 'transformers.models.fuyu.configuration_fuyu.FuyuConfig'>, <class 'transformers.models.gemma.configuration_gemma.GemmaConfig'>, <class 'transformers.models.git.configuration_git.GitConfig'>, <class 'transformers.models.gpt2.configuration_gpt2.GPT2Config'>, <class 'transformers.models.gpt2.configuration_gpt2.GPT2Config'>, <class 'transformers.models.gpt_bigcode.configuration_gpt_bigcode.GPTBigCodeConfig'>, <class 'transformers.models.gpt_neo.configuration_gpt_neo.GPTNeoConfig'>, <class 'transformers.models.gpt_neox.configuration_gpt_neox.GPTNeoXConfig'>, <class 'transformers.models.gpt_neox_japanese.configuration_gpt_neox_japanese.GPTNeoXJapaneseConfig'>, <class 'transformers.models.gptj.configuration_gptj.GPTJConfig'>, <class 'transformers.models.llama.configuration_llama.LlamaConfig'>, <class 'transformers.models.marian.configuration_marian.MarianConfig'>, <class 'transformers.models.mbart.configuration_mbart.MBartConfig'>, <class 'transformers.models.mega.configuration_mega.MegaConfig'>, <class 'transformers.models.megatron_bert.configuration_megatron_bert.MegatronBertConfig'>, <class 'transformers.models.mistral.configuration_mistral.MistralConfig'>, <class 'transformers.models.mixtral.configuration_mixtral.MixtralConfig'>, <class 'transformers.models.mpt.configuration_mpt.MptConfig'>, <class 'transformers.models.musicgen.configuration_musicgen.MusicgenConfig'>, <class 'transformers.models.mvp.configuration_mvp.MvpConfig'>, <class 'transformers.models.deprecated.open_llama.configuration_open_llama.OpenLlamaConfig'>, <class 'transformers.models.openai.configuration_openai.OpenAIGPTConfig'>, <class 'transformers.models.opt.configuration_opt.OPTConfig'>, <class 'transformers.models.pegasus.configuration_pegasus.PegasusConfig'>, <class 'transformers.models.persimmon.configuration_persimmon.PersimmonConfig'>, <class 'transformers.models.phi.configuration_phi.PhiConfig'>, <class 'transformers.models.plbart.configuration_plbart.PLBartConfig'>, <class 'transformers.models.prophetnet.configuration_prophetnet.ProphetNetConfig'>, <class 'transformers.models.qdqbert.configuration_qdqbert.QDQBertConfig'>, <class 'transformers.models.qwen2.configuration_qwen2.Qwen2Config'>, <class 'transformers.models.reformer.configuration_reformer.ReformerConfig'>, <class 'transformers.models.rembert.configuration_rembert.RemBertConfig'>, <class 'transformers.models.roberta.configuration_roberta.RobertaConfig'>, <class 'transformers.models.roberta_prelayernorm.configuration_roberta_prelayernorm.RobertaPreLayerNormConfig'>, <class 'transformers.models.roc_bert.configuration_roc_bert.RoCBertConfig'>, <class 'transformers.models.roformer.configuration_roformer.RoFormerConfig'>, <class 'transformers.models.rwkv.configuration_rwkv.RwkvConfig'>, <class 'transformers.models.speech_to_text_2.configuration_speech_to_text_2.Speech2Text2Config'>, <class 'transformers.models.stablelm.configuration_stablelm.StableLmConfig'>, <class 'transformers.models.deprecated.transfo_xl.configuration_transfo_xl.TransfoXLConfig'>, <class 'transformers.models.trocr.configuration_trocr.TrOCRConfig'>, <class 'transformers.models.whisper.configuration_whisper.WhisperConfig'>, <class 'transformers.models.xglm.configuration_xglm.XGLMConfig'>, <class 'transformers.models.xlm.configuration_xlm.XLMConfig'>, <class 'transformers.models.xlm_prophetnet.configuration_xlm_prophetnet.XLMProphetNetConfig'>, <class 'transformers.models.xlm_roberta.configuration_xlm_roberta.XLMRobertaConfig'>, <class 'transformers.models.xlm_roberta_xl.configuration_xlm_roberta_xl.XLMRobertaXLConfig'>, <class 'transformers.models.xlnet.configuration_xlnet.XLNetConfig'>, <class 'transformers.models.xmod.configuration_xmod.XmodConfig'>]
MODEL_TYPES: ('bart', 'bert', 'bert-generation', 'big_bird', 'bigbird_pegasus', 'biogpt', 'blenderbot', 'blenderbot-small', 'bloom', 'camembert', 'llama', 'codegen', 'cpmant', 'ctrl', 'data2vec-text', 'electra', 'ernie', 'falcon', 'fuyu', 'gemma', 'git', 'gpt2', 'gpt2', 'gpt_bigcode', 'gpt_neo', 'gpt_neox', 'gpt_neox_japanese', 'gptj', 'llama', 'marian', 'mbart', 'mega', 'megatron-bert', 'mistral', 'mixtral', 'mpt', 'musicgen', 'mvp', 'open-llama', 'openai-gpt', 'opt', 'pegasus', 'persimmon', 'phi', 'plbart', 'prophetnet', 'qdqbert', 'qwen2', 'reformer', 'rembert', 'roberta', 'roberta-prelayernorm', 'roc_bert', 'roformer', 'rwkv', 'speech_to_text_2', 'stablelm', 'transfo-xl', 'trocr', 'whisper', 'xglm', 'xlm', 'xlm-prophetnet', 'xlm-roberta', 'xlm-roberta-xl', 'xlnet', 'xmod')

model_args: ModelArguments(
    model_name_or_path='openai-community/gpt2',
    model_type=None,
    config_overrides=None,
    config_name=None,
    tokenizer_name=None,
    cache_dir=None,
    use_fast_tokenizer=True,
    model_revision='main',
    token=None,
    use_auth_token=None,
    trust_remote_code=False,
    torch_dtype=None,
    low_cpu_mem_usage=False
)

data_args: DataTrainingArguments(
    dataset_name='wikitext',
    dataset_config_name='wikitext-2-raw-v1',
    train_file=None,
    validation_file=None,
    max_train_samples=None,
    max_eval_samples=None,
    streaming=False,
    block_size=None,
    overwrite_cache=False,
    validation_split_percentage=5,
    preprocessing_num_workers=None,
    keep_linebreaks=True
)

training_args: TrainingArguments(
    _n_gpu=0,
    accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
    adafactor=False,
    adam_beta1=0.9,
    adam_beta2=0.999,
    adam_epsilon=1e-08,
    auto_find_batch_size=False,
    bf16=False,
    bf16_full_eval=False,
    data_seed=None,
    dataloader_drop_last=False,
    dataloader_num_workers=0,
    dataloader_persistent_workers=False,
    dataloader_pin_memory=True,
    dataloader_prefetch_factor=None,
    ddp_backend=None,
    ddp_broadcast_buffers=None,
    ddp_bucket_cap_mb=None,
    ddp_find_unused_parameters=None,
    ddp_timeout=1800,
    debug=[],
    deepspeed=None,
    disable_tqdm=False,
    dispatch_batches=None,
    do_eval=True,
    do_predict=False,
    do_train=True,
    eval_accumulation_steps=None,
    eval_delay=0,
    eval_steps=None,
    evaluation_strategy=no,
    fp16=False,
    fp16_backend=auto,
    fp16_full_eval=False,
    fp16_opt_level=O1,
    fsdp=[],
    fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
    fsdp_min_num_params=0,
    fsdp_transformer_layer_cls_to_wrap=None,
    full_determinism=False,
    gradient_accumulation_steps=1,
    gradient_checkpointing=False,
    gradient_checkpointing_kwargs=None,
    greater_is_better=None,
    group_by_length=False,
    half_precision_backend=auto,
    hub_always_push=False,
    hub_model_id=None,
    hub_private_repo=False,
    hub_strategy=every_save,
    hub_token=<HUB_TOKEN>,
    ignore_data_skip=False,
    include_inputs_for_metrics=False,
    include_num_input_tokens_seen=False,
    include_tokens_per_second=False,
    jit_mode_eval=False,
    label_names=None,
    label_smoothing_factor=0.0,
    learning_rate=5e-05,
    length_column_name=length,
    load_best_model_at_end=False,
    local_rank=0,
    log_level=passive,
    log_level_replica=warning,
    log_on_each_node=True,
    logging_dir=/tmp/test-clm/runs/Aug19_09-27-17_codespaces-322a5e,
    logging_first_step=False,
    logging_nan_inf_filter=True,
    logging_steps=500,
    logging_strategy=steps,
    lr_scheduler_kwargs={},
    lr_scheduler_type=linear,
    max_grad_norm=1.0,
    max_steps=-1,
    metric_for_best_model=None,
    mp_parameters=,
    neftune_noise_alpha=None,
    no_cuda=False,
    num_train_epochs=3.0,
    optim=adamw_torch,
    optim_args=None,
    output_dir=/tmp/test-clm,
    overwrite_output_dir=False,
    past_index=-1,
    per_device_eval_batch_size=8,
    per_device_train_batch_size=8,
    prediction_loss_only=False,
    push_to_hub=False,
    push_to_hub_model_id=None,
    push_to_hub_organization=None,
    push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
    ray_scope=last,
    remove_unused_columns=True,
    report_to=['tensorboard', 'codecarbon'],
    resume_from_checkpoint=None,
    run_name=/tmp/test-clm,
    save_on_each_node=False,
    save_only_model=False,
    save_safetensors=True,
    save_steps=500,
    save_strategy=steps,
    save_total_limit=None,
    seed=42,
    skip_memory_metrics=True,
    split_batches=None,
    tf32=None,
    torch_compile=False,
    torch_compile_backend=None,
    torch_compile_mode=None,
    torchdynamo=None,
    tpu_metrics_debug=False,
    tpu_num_cores=None,
    use_cpu=False,
    use_ipex=False,
    use_legacy_prediction_loop=False,
    use_mps_device=False,
    warmup_ratio=0.0,
    warmup_steps=0,
    weight_decay=0.0,
)

08/19/2024 10:23:38 - WARNING - __main__ - Process rank: 0, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False
08/19/2024 10:23:38 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=0,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/tmp/test-clm3/runs/Aug19_10-23-38_codespaces-322a5e,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/tmp/test-clm3,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard', 'codecarbon'],
resume_from_checkpoint=None,
run_name=/tmp/test-clm3,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
hf://datasets/wikitext@b08601e04326c79dfdd32d625aee71d232d685c3/README.md not found in cache or force_download set to True, downloading to /home/codespace/.cache/huggingface/datasets/downloads/d782d8e09126a8b9159d20854e89213c4d49aa22a54ec37e62d0613de365611f.incomplete
08/19/2024 10:23:41 - INFO - datasets.utils.file_utils - hf://datasets/wikitext@b08601e04326c79dfdd32d625aee71d232d685c3/README.md not found in cache or force_download set to True, downloading to /home/codespace/.cache/huggingface/datasets/downloads/d782d8e09126a8b9159d20854e89213c4d49aa22a54ec37e62d0613de365611f.incomplete
Downloading readme: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10.5k/10.5k [00:00<00:00, 21.3kB/s]
storing hf://datasets/wikitext@b08601e04326c79dfdd32d625aee71d232d685c3/README.md in cache at /home/codespace/.cache/huggingface/datasets/downloads/d782d8e09126a8b9159d20854e89213c4d49aa22a54ec37e62d0613de365611f
08/19/2024 10:23:43 - INFO - datasets.utils.file_utils - storing hf://datasets/wikitext@b08601e04326c79dfdd32d625aee71d232d685c3/README.md in cache at /home/codespace/.cache/huggingface/datasets/downloads/d782d8e09126a8b9159d20854e89213c4d49aa22a54ec37e62d0613de365611f
creating metadata file for /home/codespace/.cache/huggingface/datasets/downloads/d782d8e09126a8b9159d20854e89213c4d49aa22a54ec37e62d0613de365611f
08/19/2024 10:23:43 - INFO - datasets.utils.file_utils - creating metadata file for /home/codespace/.cache/huggingface/datasets/downloads/d782d8e09126a8b9159d20854e89213c4d49aa22a54ec37e62d0613de365611f
Generating dataset wikitext (/home/codespace/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3)
08/19/2024 10:23:48 - INFO - datasets.builder - Generating dataset wikitext (/home/codespace/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3)
Downloading and preparing dataset wikitext/wikitext-2-raw-v1 to /home/codespace/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3...
08/19/2024 10:23:48 - INFO - datasets.builder - Downloading and preparing dataset wikitext/wikitext-2-raw-v1 to /home/codespace/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3...
hf://datasets/wikitext@b08601e04326c79dfdd32d625aee71d232d685c3/wikitext-2-raw-v1/test-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /home/codespace/.cache/huggingface/datasets/downloads/5e548f01091b2f01906ea7be35b8a1a6aa74842629dc550b7a02ca0821f1002d.incomplete
08/19/2024 10:23:49 - INFO - datasets.utils.file_utils - hf://datasets/wikitext@b08601e04326c79dfdd32d625aee71d232d685c3/wikitext-2-raw-v1/test-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /home/codespace/.cache/huggingface/datasets/downloads/5e548f01091b2f01906ea7be35b8a1a6aa74842629dc550b7a02ca0821f1002d.incomplete
Downloading data: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 733k/733k [00:00<00:00, 1.15MB/s]
storing hf://datasets/wikitext@b08601e04326c79dfdd32d625aee71d232d685c3/wikitext-2-raw-v1/test-00000-of-00001.parquet in cache at /home/codespace/.cache/huggingface/datasets/downloads/5e548f01091b2f01906ea7be35b8a1a6aa74842629dc550b7a02ca0821f1002d
08/19/2024 10:23:50 - INFO - datasets.utils.file_utils - storing hf://datasets/wikitext@b08601e04326c79dfdd32d625aee71d232d685c3/wikitext-2-raw-v1/test-00000-of-00001.parquet in cache at /home/codespace/.cache/huggingface/datasets/downloads/5e548f01091b2f01906ea7be35b8a1a6aa74842629dc550b7a02ca0821f1002d
creating metadata file for /home/codespace/.cache/huggingface/datasets/downloads/5e548f01091b2f01906ea7be35b8a1a6aa74842629dc550b7a02ca0821f1002d
08/19/2024 10:23:50 - INFO - datasets.utils.file_utils - creating metadata file for /home/codespace/.cache/huggingface/datasets/downloads/5e548f01091b2f01906ea7be35b8a1a6aa74842629dc550b7a02ca0821f1002d
hf://datasets/wikitext@b08601e04326c79dfdd32d625aee71d232d685c3/wikitext-2-raw-v1/train-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /home/codespace/.cache/huggingface/datasets/downloads/e7f32d67ecf57307f5bc9834102f2f01ad03802eea0fdfc86172a9ec98676280.incomplete
08/19/2024 10:23:50 - INFO - datasets.utils.file_utils - hf://datasets/wikitext@b08601e04326c79dfdd32d625aee71d232d685c3/wikitext-2-raw-v1/train-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /home/codespace/.cache/huggingface/datasets/downloads/e7f32d67ecf57307f5bc9834102f2f01ad03802eea0fdfc86172a9ec98676280.incomplete
Downloading data: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6.36M/6.36M [00:00<00:00, 10.8MB/s]
storing hf://datasets/wikitext@b08601e04326c79dfdd32d625aee71d232d685c3/wikitext-2-raw-v1/train-00000-of-00001.parquet in cache at /home/codespace/.cache/huggingface/datasets/downloads/e7f32d67ecf57307f5bc9834102f2f01ad03802eea0fdfc86172a9ec98676280
08/19/2024 10:23:51 - INFO - datasets.utils.file_utils - storing hf://datasets/wikitext@b08601e04326c79dfdd32d625aee71d232d685c3/wikitext-2-raw-v1/train-00000-of-00001.parquet in cache at /home/codespace/.cache/huggingface/datasets/downloads/e7f32d67ecf57307f5bc9834102f2f01ad03802eea0fdfc86172a9ec98676280
creating metadata file for /home/codespace/.cache/huggingface/datasets/downloads/e7f32d67ecf57307f5bc9834102f2f01ad03802eea0fdfc86172a9ec98676280
08/19/2024 10:23:51 - INFO - datasets.utils.file_utils - creating metadata file for /home/codespace/.cache/huggingface/datasets/downloads/e7f32d67ecf57307f5bc9834102f2f01ad03802eea0fdfc86172a9ec98676280
hf://datasets/wikitext@b08601e04326c79dfdd32d625aee71d232d685c3/wikitext-2-raw-v1/validation-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /home/codespace/.cache/huggingface/datasets/downloads/300e51c2d3e98b6cf3739090e25ea220d1a568457b1b2d97857e1cf71da48434.incomplete
08/19/2024 10:23:52 - INFO - datasets.utils.file_utils - hf://datasets/wikitext@b08601e04326c79dfdd32d625aee71d232d685c3/wikitext-2-raw-v1/validation-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /home/codespace/.cache/huggingface/datasets/downloads/300e51c2d3e98b6cf3739090e25ea220d1a568457b1b2d97857e1cf71da48434.incomplete
Downloading data: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 657k/657k [00:00<00:00, 1.31MB/s]
storing hf://datasets/wikitext@b08601e04326c79dfdd32d625aee71d232d685c3/wikitext-2-raw-v1/validation-00000-of-00001.parquet in cache at /home/codespace/.cache/huggingface/datasets/downloads/300e51c2d3e98b6cf3739090e25ea220d1a568457b1b2d97857e1cf71da48434
08/19/2024 10:23:53 - INFO - datasets.utils.file_utils - storing hf://datasets/wikitext@b08601e04326c79dfdd32d625aee71d232d685c3/wikitext-2-raw-v1/validation-00000-of-00001.parquet in cache at /home/codespace/.cache/huggingface/datasets/downloads/300e51c2d3e98b6cf3739090e25ea220d1a568457b1b2d97857e1cf71da48434
creating metadata file for /home/codespace/.cache/huggingface/datasets/downloads/300e51c2d3e98b6cf3739090e25ea220d1a568457b1b2d97857e1cf71da48434
08/19/2024 10:23:53 - INFO - datasets.utils.file_utils - creating metadata file for /home/codespace/.cache/huggingface/datasets/downloads/300e51c2d3e98b6cf3739090e25ea220d1a568457b1b2d97857e1cf71da48434
Downloading took 0.0 min
08/19/2024 10:23:53 - INFO - datasets.download.download_manager - Downloading took 0.0 min
Checksum Computation took 0.0 min
08/19/2024 10:23:53 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min
Generating test split
08/19/2024 10:23:53 - INFO - datasets.builder - Generating test split
Generating test split: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4358/4358 [00:00<00:00, 230443.48 examples/s]
Generating train split
08/19/2024 10:23:53 - INFO - datasets.builder - Generating train split
Generating train split: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 36718/36718 [00:00<00:00, 715739.83 examples/s]
Generating validation split
08/19/2024 10:23:53 - INFO - datasets.builder - Generating validation split
Generating validation split: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 3760/3760 [00:00<00:00, 377847.12 examples/s]
All the splits matched successfully.
08/19/2024 10:23:53 - INFO - datasets.utils.info_utils - All the splits matched successfully.
Dataset wikitext downloaded and prepared to /home/codespace/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3. Subsequent calls will reuse this data.
08/19/2024 10:23:53 - INFO - datasets.builder - Dataset wikitext downloaded and prepared to /home/codespace/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3. Subsequent calls will reuse this data.
config_kwargs: {'cache_dir': None, 'revision': 'main', 'token': None, 'trust_remote_code': False}
/workspaces/huggingface-transformers/.env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 665/665 [00:00<00:00, 4.83MB/s]
[INFO|configuration_utils.py:728] 2024-08-19 10:23:54,016 >> loading configuration file config.json from cache at /home/codespace/.cache/huggingface/hub/models--openai-community--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json
[INFO|configuration_utils.py:791] 2024-08-19 10:23:54,017 >> Model config GPT2Config {
  "_name_or_path": "openai-community/gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.38.2",
  "use_cache": true,
  "vocab_size": 50257
}

config: GPT2Config {
  "_name_or_path": "openai-community/gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.38.2",
  "use_cache": true,
  "vocab_size": 50257
}

tokenizer_kwargs: {'cache_dir': None, 'use_fast': True, 'revision': 'main', 'token': None, 'trust_remote_code': False}
tokenizer_config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 26.0/26.0 [00:00<00:00, 218kB/s]
[INFO|configuration_utils.py:728] 2024-08-19 10:23:54,849 >> loading configuration file config.json from cache at /home/codespace/.cache/huggingface/hub/models--openai-community--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json
[INFO|configuration_utils.py:791] 2024-08-19 10:23:54,850 >> Model config GPT2Config {
  "_name_or_path": "openai-community/gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.38.2",
  "use_cache": true,
  "vocab_size": 50257
}

vocab.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.04M/1.04M [00:00<00:00, 5.91MB/s]
merges.txt: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 456k/456k [00:00<00:00, 2.33MB/s]
tokenizer.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.36M/1.36M [00:00<00:00, 17.7MB/s]
[INFO|tokenization_utils_base.py:2046] 2024-08-19 10:23:57,263 >> loading file vocab.json from cache at /home/codespace/.cache/huggingface/hub/models--openai-community--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-08-19 10:23:57,263 >> loading file merges.txt from cache at /home/codespace/.cache/huggingface/hub/models--openai-community--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-08-19 10:23:57,263 >> loading file tokenizer.json from cache at /home/codespace/.cache/huggingface/hub/models--openai-community--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-08-19 10:23:57,263 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-08-19 10:23:57,263 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-08-19 10:23:57,263 >> loading file tokenizer_config.json from cache at /home/codespace/.cache/huggingface/hub/models--openai-community--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-08-19 10:23:57,264 >> loading configuration file config.json from cache at /home/codespace/.cache/huggingface/hub/models--openai-community--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json
[INFO|configuration_utils.py:791] 2024-08-19 10:23:57,265 >> Model config GPT2Config {
  "_name_or_path": "openai-community/gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.38.2",
  "use_cache": true,
  "vocab_size": 50257
}

tokenizer: GPT2TokenizerFast(name_or_path='openai-community/gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={
        50256: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),
}
model.safetensors: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 548M/548M [00:01<00:00, 304MB/s]
[INFO|modeling_utils.py:3257] 2024-08-19 10:23:59,395 >> loading weights file model.safetensors from cache at /home/codespace/.cache/huggingface/hub/models--openai-community--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/model.safetensors
[INFO|configuration_utils.py:845] 2024-08-19 10:23:59,402 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

[INFO|modeling_utils.py:3992] 2024-08-19 10:23:59,584 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:4000] 2024-08-19 10:23:59,584 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at openai-community/gpt2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
generation_config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 124/124 [00:00<00:00, 1.05MB/s]
[INFO|configuration_utils.py:800] 2024-08-19 10:24:00,078 >> loading configuration file generation_config.json from cache at /home/codespace/.cache/huggingface/hub/models--openai-community--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/generation_config.json
[INFO|configuration_utils.py:845] 2024-08-19 10:24:00,078 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

model: GPT2LMHeadModel(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 768)
    (wpe): Embedding(1024, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=768, out_features=50257, bias=False)
)
Running tokenizer on dataset:   0%|                                                                                                                         | 0/4358 [00:00<?, ? examples/s]Caching processed dataset at /home/codespace/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-926c8c3c8d4a2c66.arrow
08/19/2024 10:24:00 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/codespace/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-926c8c3c8d4a2c66.arrow
Running tokenizer on dataset: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4358/4358 [00:00<00:00, 5461.88 examples/s]
Running tokenizer on dataset:   0%|                                                                                                                        | 0/36718 [00:00<?, ? examples/s]Caching processed dataset at /home/codespace/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-953f1bc62208af26.arrow
08/19/2024 10:24:01 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/codespace/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-953f1bc62208af26.arrow
Running tokenizer on dataset: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 36718/36718 [00:04<00:00, 7489.60 examples/s]
Running tokenizer on dataset:   0%|                                                                                                                         | 0/3760 [00:00<?, ? examples/s]Caching processed dataset at /home/codespace/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-e74e4750ac11e8d6.arrow
08/19/2024 10:24:05 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/codespace/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-e74e4750ac11e8d6.arrow
Running tokenizer on dataset: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3760/3760 [00:00<00:00, 8238.33 examples/s]
Grouping texts in chunks of 1024:   0%|                                                                                                                     | 0/4358 [00:00<?, ? examples/s]Caching processed dataset at /home/codespace/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-4a6bbdcea3cf1346.arrow
08/19/2024 10:24:06 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/codespace/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-4a6bbdcea3cf1346.arrow
Grouping texts in chunks of 1024: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 4358/4358 [00:00<00:00, 10446.17 examples/s]
Grouping texts in chunks of 1024:   0%|                                                                                                                    | 0/36718 [00:00<?, ? examples/s]Caching processed dataset at /home/codespace/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-41ff2a732e669c18.arrow
08/19/2024 10:24:06 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/codespace/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-41ff2a732e669c18.arrow
Grouping texts in chunks of 1024: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 36718/36718 [00:02<00:00, 12684.84 examples/s]
Grouping texts in chunks of 1024:   0%|                                                                                                                     | 0/3760 [00:00<?, ? examples/s]Caching processed dataset at /home/codespace/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-56678e444a1ff885.arrow
08/19/2024 10:24:09 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/codespace/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-56678e444a1ff885.arrow
Grouping texts in chunks of 1024: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 3760/3760 [00:00<00:00, 14053.94 examples/s]
Downloading builder script: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4.20k/4.20k [00:00<00:00, 19.3MB/s]
/workspaces/huggingface-transformers/.env/lib/python3.10/site-packages/accelerate/accelerator.py:451: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
08/19/2024 10:24:11 - WARNING - codecarbon.emissions_tracker - CODECARBON : No CPU tracking mode found. Falling back on CPU constant mode.
08/19/2024 10:24:13 - WARNING - codecarbon.emissions_tracker - CODECARBON : Failed to match CPU TDP constant. Falling back on a global constant.
[INFO|trainer.py:1812] 2024-08-19 10:24:14,065 >> ***** Running training *****
[INFO|trainer.py:1813] 2024-08-19 10:24:14,065 >>   Num examples = 2,318
[INFO|trainer.py:1814] 2024-08-19 10:24:14,065 >>   Num Epochs = 3
[INFO|trainer.py:1815] 2024-08-19 10:24:14,065 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:1818] 2024-08-19 10:24:14,065 >>   Total train batch size (w. parallel, distributed & accumulation) = 8
[INFO|trainer.py:1819] 2024-08-19 10:24:14,065 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1820] 2024-08-19 10:24:14,065 >>   Total optimization steps = 870
[INFO|trainer.py:1821] 2024-08-19 10:24:14,066 >>   Number of trainable parameters = 124,439,808
  0%|                                                                                                                                                               | 0/870 [00:00<?, ?it/s]Terminated
$
