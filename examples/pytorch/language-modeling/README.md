<!---
Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

## Language model training

Fine-tuning (or training from scratch) the library models for language modeling on a text dataset for GPT, GPT-2, ALBERT, BERT, DistilBERT, RoBERTa, XLNet...
GPT and GPT-2 are trained or fine-tuned using a **causal language modeling (CLM)** loss while ALBERT, BERT, DistilBERT and RoBERTa are trained or fine-tuned using a **masked language modeling (MLM)** loss. XLNet uses **permutation language modeling (PLM)**,
you can find more information about the differences between those objectives in our [model summary](https://huggingface.co/transformers/model_summary.html).
å¾®è°ƒï¼ˆæˆ–ä»å¤´å¼€å§‹è®­ç»ƒï¼‰ç”¨äºåœ¨ GPTã€GPT-2ã€ALBERTã€BERTã€DistilBERTã€RoBERTaã€XLNet çš„æ–‡æœ¬æ•°æ®é›†ä¸Šè¿›è¡Œè¯­è¨€å»ºæ¨¡çš„åº“æ¨¡å‹...
GPT å’Œ GPT-2 ä½¿ç”¨ä»¥ä¸‹æ–¹æ³•è¿›è¡Œè®­ç»ƒæˆ–å¾®è°ƒå› æœè¯­è¨€æ¨¡å‹ (CLM) æŸå¤±ï¼Œ
è€Œ ALBERTã€BERTã€DistilBERT å’Œ RoBERTa ä½¿ç”¨æ©ç è¯­è¨€æ¨¡å‹ (MLM) æŸå¤±è¿›è¡Œè®­ç»ƒæˆ–å¾®è°ƒã€‚
XLNet ä½¿ç”¨æ’åˆ—è¯­è¨€å»ºæ¨¡ (PLM)ï¼Œ
æ‚¨å¯ä»¥åœ¨æˆ‘ä»¬çš„æ¨¡å‹æ‘˜è¦ä¸­æ‰¾åˆ°æœ‰å…³è¿™äº›ç›®æ ‡ä¹‹é—´å·®å¼‚çš„æ›´å¤šä¿¡æ¯ã€‚

There are two sets of scripts provided. The first set leverages the Trainer API. The second set with `no_trainer` in the suffix uses a custom training loop and leverages the ğŸ¤— Accelerate library . Both sets use the ğŸ¤— Datasets library. You can easily customize them to your needs if you need extra processing on your datasets.
æä¾›äº†ä¸¤ç»„è„šæœ¬ã€‚ç¬¬ä¸€ç»„åˆ©ç”¨ Trainer APIã€‚åç¼€ä¸­å¸¦æœ‰no_trainerçš„ç¬¬äºŒç»„ä½¿ç”¨è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯å¹¶åˆ©ç”¨ ğŸ¤— Accelerate åº“ã€‚ä¸¤ä¸ªé›†åˆéƒ½ä½¿ç”¨ ğŸ¤— æ•°æ®é›†åº“ã€‚å¦‚æœæ‚¨éœ€è¦å¯¹æ•°æ®é›†è¿›è¡Œé¢å¤–å¤„ç†ï¼Œæ‚¨å¯ä»¥æ ¹æ®éœ€è¦è½»æ¾è‡ªå®šä¹‰å®ƒä»¬ã€‚

**Note:** The old script `run_language_modeling.py` is still available [here](https://github.com/huggingface/transformers/blob/main/examples/legacy/run_language_modeling.py).
æ³¨æ„ï¼šæ—§è„šæœ¬run_language_modeling.pyä»ç„¶å¯ä»¥åœ¨æ­¤å¤„ä½¿ç”¨ã€‚

The following examples, will run on datasets hosted on our [hub](https://huggingface.co/datasets) or with your own
text files for training and validation. We give examples of both below.
ä»¥ä¸‹ç¤ºä¾‹å°†åœ¨æˆ‘ä»¬ä¸­å¿ƒæ‰˜ç®¡çš„æ•°æ®é›†ä¸Šè¿è¡Œï¼Œæˆ–ä½¿ç”¨æ‚¨è‡ªå·±çš„æ–‡æœ¬æ–‡ä»¶è¿›è¡Œè®­ç»ƒå’ŒéªŒè¯ã€‚æˆ‘ä»¬åœ¨ä¸‹é¢ç»™å‡ºäº†ä¸¤è€…çš„ä¾‹å­ã€‚

### GPT-2/GPT and causal language modeling

The following example fine-tunes GPT-2 on WikiText-2. We're using the raw WikiText-2 (no tokens were replaced before
the tokenization). The loss here is that of causal language modeling.
ä»¥ä¸‹ç¤ºä¾‹åœ¨ WikiText-2 ä¸Šå¾®è°ƒ GPT-2ã€‚æˆ‘ä»¬ä½¿ç”¨åŸå§‹çš„ WikiText-2ï¼ˆåœ¨æ ‡è®°åŒ–ä¹‹å‰æ²¡æœ‰æ›¿æ¢ä»»ä½•æ ‡è®°ï¼‰ã€‚è¿™é‡Œçš„æŸå¤±æ˜¯å› æœè¯­è¨€å»ºæ¨¡çš„æŸå¤±ã€‚

```bash
python run_clm.py \
    --model_name_or_path openai-community/gpt2 \
    --dataset_name wikitext \
    --dataset_config_name wikitext-2-raw-v1 \
    --per_device_train_batch_size 8 \
    --per_device_eval_batch_size 8 \
    --do_train \
    --do_eval \
    --output_dir /tmp/test-clm
```

This takes about half an hour to train on a single K80 GPU and about one minute for the evaluation to run. It reaches
a score of ~20 perplexity once fine-tuned on the dataset.
åœ¨å•ä¸ª K80 GPU ä¸Šè®­ç»ƒå¤§çº¦éœ€è¦åŠå°æ—¶ï¼Œè¿è¡Œè¯„ä¼°å¤§çº¦éœ€è¦ä¸€åˆ†é’Ÿã€‚ä¸€æ—¦å¯¹æ•°æ®é›†è¿›è¡Œå¾®è°ƒï¼Œå®ƒçš„å›°æƒ‘åº¦åˆ†æ•°å°±ä¼šè¾¾åˆ°çº¦ 20ã€‚

To run on your own training and validation files, use the following command:
è¦åœ¨æ‚¨è‡ªå·±çš„è®­ç»ƒå’ŒéªŒè¯æ–‡ä»¶ä¸Šè¿è¡Œï¼Œè¯·ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ï¼š

```bash
python run_clm.py \
    --model_name_or_path openai-community/gpt2 \
    --train_file path_to_train_file \
    --validation_file path_to_validation_file \
    --per_device_train_batch_size 8 \
    --per_device_eval_batch_size 8 \
    --do_train \
    --do_eval \
    --output_dir /tmp/test-clm
```

This uses the built in HuggingFace `Trainer` for training. If you want to use a custom training loop, you can utilize or adapt the `run_clm_no_trainer.py` script. Take a look at the script for a list of supported arguments. An example is shown below:
è¿™ä½¿ç”¨å†…ç½®çš„ HuggingFace Trainerè¿›è¡Œè®­ç»ƒã€‚å¦‚æœæ‚¨æƒ³ä½¿ç”¨è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ï¼Œæ‚¨å¯ä»¥åˆ©ç”¨æˆ–æ”¹ç¼–run_clm_no_trainer.pyè„šæœ¬ã€‚æŸ¥çœ‹è„šæœ¬ä»¥è·å–æ”¯æŒçš„å‚æ•°åˆ—è¡¨ã€‚ç¤ºä¾‹å¦‚ä¸‹æ‰€ç¤ºï¼š

```bash
python run_clm_no_trainer.py \
    --dataset_name wikitext \
    --dataset_config_name wikitext-2-raw-v1 \
    --model_name_or_path openai-community/gpt2 \
    --output_dir /tmp/test-clm
```

### RoBERTa/BERT/DistilBERT and masked language modeling

The following example fine-tunes RoBERTa on WikiText-2. Here too, we're using the raw WikiText-2. The loss is different
as BERT/RoBERTa have a bidirectional mechanism; we're therefore using the same loss that was used during their
pre-training: masked language modeling.
ä»¥ä¸‹ç¤ºä¾‹å¯¹ WikiText-2 ä¸Šçš„ RoBERTa è¿›è¡Œå¾®è°ƒã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä¹Ÿä½¿ç”¨åŸå§‹çš„ WikiText-2ã€‚ç”±äºBERT/RoBERTaå…·æœ‰åŒå‘æœºåˆ¶ï¼ŒæŸå¤±ä¸åŒï¼›å› æ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸é¢„è®­ç»ƒæœŸé—´ä½¿ç”¨çš„ç›¸åŒæŸå¤±ï¼šæ©ç è¯­è¨€å»ºæ¨¡ã€‚

In accordance to the RoBERTa paper, we use dynamic masking rather than static masking. The model may, therefore,
converge slightly slower (over-fitting takes more epochs).
æ ¹æ® RoBERTa è®ºæ–‡ï¼Œæˆ‘ä»¬ä½¿ç”¨åŠ¨æ€æ©ç è€Œä¸æ˜¯é™æ€æ©ç ã€‚å› æ­¤ï¼Œæ¨¡å‹çš„æ”¶æ•›é€Ÿåº¦å¯èƒ½ä¼šç¨æ…¢ï¼ˆè¿‡åº¦æ‹Ÿåˆéœ€è¦æ›´å¤šçš„æ—¶é—´ï¼‰ã€‚

```bash
python run_mlm.py \
    --model_name_or_path FacebookAI/roberta-base \
    --dataset_name wikitext \
    --dataset_config_name wikitext-2-raw-v1 \
    --per_device_train_batch_size 8 \
    --per_device_eval_batch_size 8 \
    --do_train \
    --do_eval \
    --output_dir /tmp/test-mlm
```

To run on your own training and validation files, use the following command:
è¦åœ¨æ‚¨è‡ªå·±çš„è®­ç»ƒå’ŒéªŒè¯æ–‡ä»¶ä¸Šè¿è¡Œï¼Œè¯·ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ï¼š

```bash
python run_mlm.py \
    --model_name_or_path FacebookAI/roberta-base \
    --train_file path_to_train_file \
    --validation_file path_to_validation_file \
    --per_device_train_batch_size 8 \
    --per_device_eval_batch_size 8 \
    --do_train \
    --do_eval \
    --output_dir /tmp/test-mlm
```

If your dataset is organized with one sample per line, you can use the `--line_by_line` flag (otherwise the script
concatenates all texts and then splits them in blocks of the same length).
å¦‚æœæ‚¨çš„æ•°æ®é›†æŒ‰æ¯è¡Œä¸€ä¸ªæ ·æœ¬è¿›è¡Œç»„ç»‡ï¼Œåˆ™å¯ä»¥ä½¿ç”¨--line_by_lineæ ‡å¿—ï¼ˆå¦åˆ™è„šæœ¬ä¼šè¿æ¥æ‰€æœ‰æ–‡æœ¬ï¼Œç„¶åå°†å®ƒä»¬æ‹†åˆ†ä¸ºç›¸åŒé•¿åº¦çš„å—ï¼‰ã€‚

This uses the built in HuggingFace `Trainer` for training. If you want to use a custom training loop, you can utilize or adapt the `run_mlm_no_trainer.py` script. Take a look at the script for a list of supported arguments. An example is shown below:
è¿™ä½¿ç”¨å†…ç½®çš„ HuggingFace Trainerè¿›è¡Œè®­ç»ƒã€‚å¦‚æœæ‚¨æƒ³ä½¿ç”¨è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ï¼Œæ‚¨å¯ä»¥åˆ©ç”¨æˆ–æ”¹ç¼–run_mlm_no_trainer.pyè„šæœ¬ã€‚æŸ¥çœ‹è„šæœ¬ä»¥è·å–æ”¯æŒçš„å‚æ•°åˆ—è¡¨ã€‚ç¤ºä¾‹å¦‚ä¸‹æ‰€ç¤ºï¼š

```bash
python run_mlm_no_trainer.py \
    --dataset_name wikitext \
    --dataset_config_name wikitext-2-raw-v1 \
    --model_name_or_path FacebookAI/roberta-base \
    --output_dir /tmp/test-mlm
```

**Note:** On TPU, you should use the flag `--pad_to_max_length` in conjunction with the `--line_by_line` flag to make
sure all your batches have the same length.
æ³¨æ„ï¼šåœ¨ TPU ä¸Šï¼Œæ‚¨åº”è¯¥å°†--pad_to_max_lengthæ ‡å¿—ä¸--line_by_lineæ ‡å¿—ç»“åˆä½¿ç”¨ï¼Œä»¥ç¡®ä¿æ‰€æœ‰æ‰¹æ¬¡å…·æœ‰ç›¸åŒçš„é•¿åº¦ã€‚

### Whole word masking

This part was moved to `examples/research_projects/mlm_wwm`.

### XLNet and permutation language modeling

XLNet uses a different training objective, which is permutation language modeling. It is an autoregressive method
to learn bidirectional contexts by maximizing the expected likelihood over all permutations of the input
sequence factorization order.
XLNet ä½¿ç”¨ä¸åŒçš„è®­ç»ƒç›®æ ‡ï¼Œå³æ’åˆ—è¯­è¨€å»ºæ¨¡ã€‚å®ƒæ˜¯ä¸€ç§è‡ªå›å½’æ–¹æ³•ï¼Œé€šè¿‡æœ€å¤§åŒ–è¾“å…¥åºåˆ—åˆ†è§£é¡ºåºçš„æ‰€æœ‰æ’åˆ—çš„é¢„æœŸä¼¼ç„¶æ¥å­¦ä¹ åŒå‘ä¸Šä¸‹æ–‡ã€‚

We use the `--plm_probability` flag to define the ratio of length of a span of masked tokens to surrounding
context length for permutation language modeling.
æˆ‘ä»¬ä½¿ç”¨--plm_probabilityæ ‡å¿—æ¥å®šä¹‰å±è”½æ ‡è®°èŒƒå›´çš„é•¿åº¦ä¸å‘¨å›´ä¸Šä¸‹æ–‡é•¿åº¦çš„æ¯”ç‡ï¼Œä»¥è¿›è¡Œæ’åˆ—è¯­è¨€å»ºæ¨¡ã€‚

The `--max_span_length` flag may also be used to limit the length of a span of masked tokens used
for permutation language modeling.
--max_span_lengthæ ‡å¿—è¿˜å¯ç”¨äºé™åˆ¶ç”¨äºæ’åˆ—è¯­è¨€å»ºæ¨¡çš„å±è”½æ ‡è®°çš„è·¨åº¦çš„é•¿åº¦ã€‚

Here is how to fine-tune XLNet on wikitext-2:
ä»¥ä¸‹æ˜¯å¦‚ä½•åœ¨ wikitext-2 ä¸Šå¾®è°ƒ XLNetï¼š

```bash
python run_plm.py \
    --model_name_or_path=xlnet/xlnet-base-cased \
    --dataset_name wikitext \
    --dataset_config_name wikitext-2-raw-v1 \
    --per_device_train_batch_size 8 \
    --per_device_eval_batch_size 8 \
    --do_train \
    --do_eval \
    --output_dir /tmp/test-plm
```

To fine-tune it on your own training and validation file, run:
è¦åœ¨æ‚¨è‡ªå·±çš„è®­ç»ƒå’ŒéªŒè¯æ–‡ä»¶ä¸Šå¯¹å…¶è¿›è¡Œå¾®è°ƒï¼Œè¯·è¿è¡Œï¼š

```bash
python run_plm.py \
    --model_name_or_path=xlnet/xlnet-base-cased \
    --train_file path_to_train_file \
    --validation_file path_to_validation_file \
    --per_device_train_batch_size 8 \
    --per_device_eval_batch_size 8 \
    --do_train \
    --do_eval \
    --output_dir /tmp/test-plm
```

If your dataset is organized with one sample per line, you can use the `--line_by_line` flag (otherwise the script
concatenates all texts and then splits them in blocks of the same length).
å¦‚æœæ‚¨çš„æ•°æ®é›†æŒ‰æ¯è¡Œä¸€ä¸ªæ ·æœ¬è¿›è¡Œç»„ç»‡ï¼Œåˆ™å¯ä»¥ä½¿ç”¨--line_by_lineæ ‡å¿—ï¼ˆå¦åˆ™è„šæœ¬ä¼šè¿æ¥æ‰€æœ‰æ–‡æœ¬ï¼Œç„¶åå°†å®ƒä»¬æ‹†åˆ†ä¸ºç›¸åŒé•¿åº¦çš„å—ï¼‰ã€‚

**Note:** On TPU, you should use the flag `--pad_to_max_length` in conjunction with the `--line_by_line` flag to make
sure all your batches have the same length.
æ³¨æ„ï¼šåœ¨ TPU ä¸Šï¼Œæ‚¨åº”è¯¥å°†--pad_to_max_lengthæ ‡å¿—ä¸--line_by_lineæ ‡å¿—ç»“åˆä½¿ç”¨ï¼Œä»¥ç¡®ä¿æ‰€æœ‰æ‰¹æ¬¡å…·æœ‰ç›¸åŒçš„é•¿åº¦ã€‚

## Streaming

To use the streaming dataset mode which can be very useful for large datasets, add `--streaming` to the command line. This is currently supported by `run_mlm.py` and `run_clm.py`.
è¦ä½¿ç”¨å¯¹å¤§å‹æ•°æ®é›†éå¸¸æœ‰ç”¨çš„æµæ•°æ®é›†æ¨¡å¼ï¼Œè¯·å°†--streamingæ·»åŠ åˆ°å‘½ä»¤è¡Œã€‚ç›®å‰run_mlm.pyå’Œrun_clm.pyæ”¯æŒæ­¤åŠŸèƒ½ã€‚

## Low Cpu Memory Usage

To use low cpu memory mode which can be very useful for LLM, add `--low_cpu_mem_usage` to the command line. This is currently supported by `run_clm.py`,`run_mlm.py`, `run_plm.py`,`run_mlm_no_trainer.py` and `run_clm_no_trainer.py`.
ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹æ—¶ï¼Œå¯ä»¥åœ¨--config_overridesçš„å¸®åŠ©ä¸‹è¦†ç›–é…ç½®å€¼ï¼š

## Creating a model on the fly

When training a model from scratch, configuration values may be overridden with the help of `--config_overrides`:


```bash
python run_clm.py --model_type openai-community/gpt2 --tokenizer_name openai-community/gpt2 \ --config_overrides="n_embd=1024,n_head=16,n_layer=48,n_positions=102" \
[...]
```

This feature is only available in `run_clm.py`, `run_plm.py` and `run_mlm.py`.
æ­¤åŠŸèƒ½ä»…åœ¨run_clm.py ã€ run_plm.pyå’Œrun_mlm.pyä¸­å¯ç”¨ã€‚
